# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z6_LbPCjIg3zEKpmFL2IKgHrkrtdj0ih
"""

# Importing the libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix, accuracy_score
import xgboost as xgb  # For Gradient Boosting with XGBoost

# Importing the dataset
datasets = pd.read_csv("diabetes.csv")
X = datasets.iloc[:, :-1].values  # Features
y = datasets.iloc[:, -1].values   # Target variable
# Displaying the first few rows of the dataset
print(datasets.head())
print(datasets.describe())
print(datasets.shape)

# Splitting the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)
print(X.shape, X_train.shape, X_test.shape)


# Feature Scaling
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)  # Use transform instead of fit_transform to avoid data leakage

# Fitting models and evaluating results
models = {
    'SVM (linear)': SVC(kernel="linear", random_state=0),
    'Logistic Regression': LogisticRegression(random_state=0),
    'KNN': KNeighborsClassifier(n_neighbors=5, p=2, metric="minkowski"),
    'SVM (RBF)': SVC(kernel="rbf", random_state=0),
    'Naive Bayes': GaussianNB(),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=0),
    'Decision Tree': DecisionTreeClassifier(random_state=0),
    'Gradient Boosting (XGBoost)': xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
}
model_accuracies = {}

for model_name, model in models.items():
    print(f"\nTraining and evaluating {model_name}...")

    # Fit the model
    model.fit(X_train, y_train)

    # Predict the test results
    y_predict = model.predict(X_test)

    # Evaluating the model
    cm = confusion_matrix(y_test, y_predict)
    accuracy = accuracy_score(y_test, y_predict)

    print(f"Confusion Matrix for {model_name}:\n{cm}")
    print(f"Accuracy for {model_name}: {accuracy:.4f}")

    # Store the accuracy in the dictionary # This line was moved inside the loop
    model_accuracies[model_name] = accuracy


# Identifying the best model based on accuracy
best_model_name = max(model_accuracies, key=model_accuracies.get)
best_model_accuracy = model_accuracies[best_model_name]

print(f"\nThe best model is {best_model_name} with an accuracy of {best_model_accuracy:.4f}")

# Predicting a new result with Naive Bayes (or any other model)
input_data = (5, 116, 74, 0, 0, 25.6, 0.201, 30)
input_data_as_numpy_array = np.asarray(input_data)
input_data_reshaped = input_data_as_numpy_array.reshape(1, -1)

# Standardize the input data
std_data = sc.transform(input_data_reshaped)

# Make prediction
model = models['Naive Bayes']  # You can choose any other model here
prediction = model.predict(std_data)

# Display the result
if prediction[0] == 0:
    print("\nThe patient is not diabetic.")
else:
    print("\nThe patient is diabetic.")

# Identifying the best model based on accuracy
best_model_name = max(model_accuracies, key=model_accuracies.get)
best_model_accuracy = model_accuracies[best_model_name]

print(f"\nThe best model is {best_model_name} with an accuracy of {best_model_accuracy:.4f}")